[package]
name = "rs-airllm"
version = "0.1.0"
edition = "2024"
description = "Memory-efficient LLM inference - run 70B+ models on limited GPU memory"
license = "MIT"
keywords = ["llm", "inference", "memory-efficient", "transformers"]
categories = ["science", "machine-learning"]

[dependencies]
# Tensor operations
candle-core = "0.9"
candle-nn = "0.9"
candle-transformers = "0.9"

# Tokenization
tokenizers = "0.22"

# Model file loading
safetensors = "0.7"
memmap2 = "0.9"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Async runtime for prefetching
tokio = { version = "1.0", features = ["rt-multi-thread", "sync", "fs", "macros"] }

# HTTP client for model downloads
reqwest = { version = "0.13", default-features = false, features = ["json", "stream", "rustls"] }

# CLI
clap = { version = "4.0", features = ["derive", "env"] }

# Progress bars
indicatif = "0.18"

# Error handling
thiserror = "2.0"
anyhow = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Utilities
byteorder = "1.5"
half = { version = "2.4", features = ["num-traits", "serde"] }
num-traits = "0.2"
rand = "0.9"

[features]
default = []
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]

[[bin]]
name = "airllm"
path = "src/main.rs"

[profile.release]
lto = true
codegen-units = 1
opt-level = 3
